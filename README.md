## 🚀 Internship Tasks – Branches

| Task | Branch | Description |
|------|--------|-------------|
| ✅ Task 1 – Population Data Visualization | [task-01](https://github.com/Shivangi-106/Prodigy-Infotech-Internship/tree/Prodigy_DS_01) |Created bar charts and histograms to visualize the distribution of age and gender data using World Bank statistics. |
| ✅ Task 2 – Titanic Data Cleaning & EDA | [task-02](https://github.com/Shivangi-106/Prodigy-Infotech-Internship/tree/Prodigy_DS_02) | Performed extensive data cleaning and exploratory analysis on the Titanic dataset to explore survival patterns and relationships. |
| ✅ Task 3 – Customer Purchase Prediction | [task-03](https://github.com/Shivangi-106/Prodigy-Infotech-Internship/tree/Prodigy_DS_03) | Built a decision tree classifier on UCI Bank Marketing data to predict product subscription based on customer demographics and behavior. |
| ✅ Task 4 – Twitter Sentiment analysis | [task-04](https://github.com/Shivangi-106/Prodigy-Infotech-Internship/tree/Prodigy_DS_04) | Analyzed social media sentiment from tweets to understand public opinion using entity-level sentiment classification. |
| ✅ Task 5 – Traffic Accident Pattern Analysis | [task-05](https://github.com/Shivangi-106/Prodigy-Infotech-Internship/tree/Prodigy_DS_05) | Explored and visualized US traffic accident data to identify accident hotspots and contributing factors like weather and time of day. |
📄 [Internship Report](internship_report.md)





# 🧠 Prodigy Infotech Data Science Internship Projects

**Internship Role:** Data Science Intern
**Duration:** June 2025 
**Organization:** Prodigy Infotech (https://prodigyinfotech.dev/)

Welcome to my project repository completed during my **Data Science Internship** at **Prodigy Infotech**. This internship gave me practical exposure to core data science concepts including **data visualization, data cleaning, exploratory data analysis, classification models, sentiment analysis**, and **geo-visualization**.

This repository contains **five major tasks** assigned during the internship. Each task focuses on applying real-world datasets and implementing hands-on solutions using Python, Pandas, Seaborn, Scikit-learn, and other relevant libraries.

---

## 🗂️ Table of Contents

* [Task 01 – Population Visualization](#task-01--population-visualization)
* [Task 02 – Titanic Data EDA](#task-02--titanic-data-eda)
* [Task 03 – Purchase Prediction with Decision Trees](#task-03--purchase-prediction-with-decision-trees)
* [Task 04 – Twitter Sentiment Analysis](#task-04--twitter-sentiment-analysis)
* [Task 05 – US Traffic Accident Analysis](#task-05--us-traffic-accident-analysis)
* [🔍 Key Learnings](#-key-learnings)
* [📌 Final Note](#-final-note)

---

## 📊 Task 01 – Population Visualization

**Objective:**
To visualize the distribution of a population variable (e.g., age or gender) using a bar chart or histogram.

**Dataset Used:**
[World Bank Population Dataset](https://data.worldbank.org/indicator/SP.POP.TOTL)

**What I Did:**

* Loaded and cleaned global population data.
* Visualized country-wise population distribution using bar charts and histograms.
* Enhanced plot aesthetics with Seaborn and Matplotlib.

**Skills Used:**
`pandas`, `matplotlib`, `seaborn`, `data visualization`

---

## 🔍 Task 02 – Titanic Data EDA

**Objective:**
Perform data cleaning and exploratory data analysis on the Titanic dataset to uncover survival patterns.

**Dataset Used:**
[Kaggle Titanic Dataset](https://www.kaggle.com/c/titanic/data)

**What I Did:**

* Cleaned missing values and categorized features.
* Explored survival rates based on age, gender, class, and family size.
* Visualized trends using heatmaps, pie charts, and bar plots.

**Skills Used:**
`pandas`, `numpy`, `seaborn`, `EDA`, `feature engineering`

---

## 🌳 Task 03 – Purchase Prediction with Decision Trees

**Objective:**
Build a decision tree classifier to predict whether a customer will purchase a product or service based on their profile.

**Dataset Used:**
[Bank Marketing Dataset – UCI Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)

**What I Did:**

* Preprocessed customer demographic and campaign response data.
* Built a Decision Tree Classifier to predict purchasing behavior.
* Evaluated model performance using accuracy, precision, and confusion matrix.

**Skills Used:**
`scikit-learn`, `decision trees`, `label encoding`, `model evaluation`, `classification`

---

## 💬 Task 04 – Twitter Sentiment Analysis

**Objective:**
Analyze and visualize sentiment patterns in tweets related to specific entities or topics.

**Dataset Used:**
[Twitter Entity Sentiment Dataset](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)

**What I Did:**

* Cleaned and tokenized tweet text.
* Performed sentiment analysis using VADER/TextBlob.
* Created word clouds and sentiment distribution plots.

**Skills Used:**
`nltk`, `TextBlob`, `wordcloud`, `sentiment analysis`, `natural language processing`

---

## 🚦 Task 05 – US Traffic Accident Analysis

**Objective:**
Analyze traffic accident data to find patterns based on location, time, and weather conditions.

**Dataset Used:**
[US Accidents Dataset (Kaggle)](https://www.kaggle.com/code/harshalbhamare/us-accident-eda)

**What I Did:**

* Analyzed over 2 million accident records.
* Visualized accident hotspots, weather impact, and time-based trends.
* Created heatmaps, scatter plots, and temporal distribution charts.

**Skills Used:**
`pandas`, `matplotlib`, `seaborn`, `folium`, `datetime`, `geospatial analysis`

---

## 📘 Key Learnings

Throughout this internship, I gained:

* **Strong understanding of EDA** and how to derive insights from raw data.
* **Hands-on practice with data preprocessing** and model building using industry datasets.
* Experience with **visual storytelling using graphs, charts, and maps.**
* Understanding of **real-world challenges** in classification, text processing, and data cleaning.


## 📌 Final Note

This internship has been a stepping stone in applying theoretical knowledge to practical, data-driven problems. I'm excited to continue my journey in data science and explore deeper areas like machine learning, deployment, and advanced analytics.
